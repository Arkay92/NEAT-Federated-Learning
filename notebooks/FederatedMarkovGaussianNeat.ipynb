{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install python3-dev python3-pip python3-setuptools gcc libffi-dev libssl-dev\n",
        "\n",
        "!pip install swig\n",
        "!pip install gym\n",
        "!pip install box2d-py\n",
        "!pip install gym[box2d]\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install neat-python\n",
        "\n",
        "!pip install --upgrade tensorflow_probability\n",
        "!pip install --upgrade tensorflow_model_optimization\n",
        "!pip install --upgrade tensorflow tensorflow-federated\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jU1PuUTDpqIA",
        "outputId": "4c7d6a68-5271-4733-e2c5-d960562000bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,702 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,985 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,731 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,246 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,564 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,264 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.3 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,953 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,410 kB]\n",
            "Fetched 31.9 MB in 31s (1,015 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "gcc is already the newest version (4:11.2.0-1ubuntu1).\n",
            "gcc set to manually installed.\n",
            "libffi-dev is already the newest version (3.4.2-4).\n",
            "libffi-dev set to manually installed.\n",
            "libssl-dev is already the newest version (3.0.2-0ubuntu1.19).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "python3-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  python3-pkg-resources python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "The following packages will be upgraded:\n",
            "  python3-pkg-resources\n",
            "1 upgraded, 3 newly installed, 0 to remove and 66 not upgraded.\n",
            "Need to get 2,018 kB of archives.\n",
            "After this operation, 9,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.5 [1,306 kB]\n",
            "Get:3 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-pkg-resources all 68.1.2-2~jammy3 [216 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-setuptools all 68.1.2-2~jammy3 [465 kB]\n",
            "Fetched 2,018 kB in 13s (152 kB/s)\n",
            "(Reading database ... 126109 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.5_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Setting up python3-pkg-resources (68.1.2-2~jammy3) ...\n",
            "Setting up python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting swig\n",
            "  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.1\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp311-cp311-linux_x86_64.whl size=2379368 sha256=3a81ce0794532bf7932f17f321dd366156431a3b0fe48b55a754902b21422fee\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/95/02/4cb5adc9f6dcaeb9639c2271f630a66ab4440102414804c45c\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym[box2d]) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0.tar.gz (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting neat-python\n",
            "  Downloading neat_python-0.92-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading neat_python-0.92-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neat-python\n",
            "Successfully installed neat-python-0.92\n",
            "Requirement already satisfied: tensorflow_probability in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow_probability) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_probability) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow_probability) (2.0.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow_probability) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow_probability) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow_probability) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_probability) (0.1.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_probability) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_probability) (1.17.2)\n",
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (0.1.9)\n",
            "Collecting numpy~=1.23 (from tensorflow_model_optimization)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six~=1.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (1.17.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow_model_optimization) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow_model_optimization) (1.17.2)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorflow_model_optimization\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 tensorflow_model_optimization-0.8.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.87.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated) (5.5.2)\n",
            "Collecting dm-tree==0.1.8 (from tensorflow-federated)\n",
            "  Downloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting dp-accounting==0.4.3 (from tensorflow-federated)\n",
            "  Downloading dp_accounting-0.4.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting google-vizier==0.1.11 (from tensorflow-federated)\n",
            "  Downloading google_vizier-0.1.11-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jaxlib==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jaxlib-0.4.14-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting jax==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of tensorflow-federated to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.86.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "  Downloading tensorflow_federated-0.85.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "  Downloading tensorflow_federated-0.84.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Collecting portpicker~=1.6 (from tensorflow-federated)\n",
            "  Downloading portpicker-1.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting scipy~=1.9.3 (from tensorflow-federated)\n",
            "  Downloading scipy-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-optimization==0.7.5 (from tensorflow-federated)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
            "Collecting tensorflow-privacy==0.9.0 (from tensorflow-federated)\n",
            "  Downloading tensorflow_privacy-0.9.0-py3-none-any.whl.metadata (763 bytes)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated) (4.67.1)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting googleapis-common-protos==1.61.0 (from tensorflow-federated)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.11/dist-packages (from dp-accounting==0.4.3->tensorflow-federated) (1.3.0)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting sqlalchemy<=1.4.20,>=1.4 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading SQLAlchemy-1.4.20.tar.gz (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting packaging (from tensorflow)\n",
            "  Downloading packaging-22.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated) (1.6.1)\n",
            "Collecting tensorflow-probability~=0.22.0 (from tensorflow-privacy==0.9.0->tensorflow-federated)\n",
            "  Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from portpicker~=1.6->tensorflow-federated) (5.9.5)\n",
            "Collecting numpy~=1.25 (from tensorflow-federated)\n",
            "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (2.0.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.69.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.68.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.67.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.67.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_tools-1.66.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_tools-1.65.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.62.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated) (3.2.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Downloading tensorflow_federated-0.84.0-py3-none-manylinux_2_31_x86_64.whl (71.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_vizier-0.1.11-py3-none-any.whl (721 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.14-cp311-cp311-manylinux2014_x86_64.whl (73.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_privacy-0.9.0-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n",
            "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.4/33.4 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading grpcio_tools-1.62.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.22.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jax, sqlalchemy\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535471 sha256=19c892120bb4e697b0820db47b7fae29e5f00f23fd52697ec9e2a090412542eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/8d/5d/66b1fbb551b0c3a21696015b7339b8241ebfa128bb9145febd\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.20-cp311-cp311-linux_x86_64.whl size=1531754 sha256=0df8667cd992f53f9faf9e0f353b959de24743884e26f14e73093e353ff99a85\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/a2/a2/9290912f40321acfd65cf7a741d2e0882e9a1f9464f60e2e9b\n",
            "Successfully built jax sqlalchemy\n",
            "Installing collected packages: dm-tree, wrapt, typing-extensions, tensorflow-estimator, sqlalchemy, protobuf, portpicker, packaging, numpy, keras, attrs, tensorflow-probability, tensorflow-model-optimization, scipy, ml-dtypes, grpcio-tools, googleapis-common-protos, jaxlib, jax, google-vizier, google-auth-oauthlib, dp-accounting, tensorboard, tensorflow, tensorflow-privacy, tensorflow-federated\n",
            "  Attempting uninstall: dm-tree\n",
            "    Found existing installation: dm-tree 0.1.9\n",
            "    Uninstalling dm-tree-0.1.9:\n",
            "      Successfully uninstalled dm-tree-0.1.9\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.41\n",
            "    Uninstalling SQLAlchemy-2.0.41:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.41\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: portpicker\n",
            "    Found existing installation: portpicker 1.5.2\n",
            "    Uninstalling portpicker-1.5.2:\n",
            "      Successfully uninstalled portpicker-1.5.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.3.0\n",
            "    Uninstalling attrs-25.3.0:\n",
            "      Successfully uninstalled attrs-25.3.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.25.0\n",
            "    Uninstalling tensorflow-probability-0.25.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.25.0\n",
            "  Attempting uninstall: tensorflow-model-optimization\n",
            "    Found existing installation: tensorflow-model-optimization 0.8.0\n",
            "    Uninstalling tensorflow-model-optimization-0.8.0:\n",
            "      Successfully uninstalled tensorflow-model-optimization-0.8.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.70.0\n",
            "    Uninstalling googleapis-common-protos-1.70.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.70.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, but you have scipy 1.9.3 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.9.3 which is incompatible.\n",
            "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.14 which is incompatible.\n",
            "chex 0.1.89 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, but you have jaxlib 0.4.14 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.9.3 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.14 which is incompatible.\n",
            "optax 0.2.4 requires jax>=0.4.27, but you have jax 0.4.14 which is incompatible.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, but you have jaxlib 0.4.14 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "pydantic-core 2.33.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic 2.11.4 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.14.1 which is incompatible.\n",
            "stumpy 1.13.0 requires scipy>=1.10, but you have scipy 1.9.3 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.9.3 which is incompatible.\n",
            "blosc2 3.3.3 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "sphinx 8.2.3 requires packaging>=23.0, but you have packaging 22.0 which is incompatible.\n",
            "xarray 2025.3.1 requires packaging>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.1 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 22.0 which is incompatible.\n",
            "typeguard 4.4.2 requires typing_extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "openai 1.81.0 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "google-genai 1.16.1 requires typing-extensions<5.0.0,>=4.11.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "langsmith 0.3.42 requires packaging>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "typing-inspection 0.4.1 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.14.1 which is incompatible.\n",
            "google-cloud-bigquery 3.33.0 requires packaging>=24.2.0, but you have packaging 22.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "albumentations 2.0.7 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\n",
            "langchain-core 0.3.60 requires packaging<25,>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "langchain-core 0.3.60 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 dm-tree-0.1.8 dp-accounting-0.4.3 google-auth-oauthlib-1.0.0 google-vizier-0.1.11 googleapis-common-protos-1.61.0 grpcio-tools-1.62.3 jax-0.4.14 jaxlib-0.4.14 keras-2.14.0 ml-dtypes-0.2.0 numpy-1.25.2 packaging-22.0 portpicker-1.6.0 protobuf-4.25.8 scipy-1.9.3 sqlalchemy-1.4.20 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-federated-0.84.0 tensorflow-model-optimization-0.7.5 tensorflow-privacy-0.9.0 tensorflow-probability-0.22.1 typing-extensions-4.5.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "portpicker"
                ]
              },
              "id": "5f74ee1186b94ff2a8c10ed6a2fd3582"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (22.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.2\n",
            "    Uninstalling typeguard-4.4.2:\n",
            "      Successfully uninstalled typeguard-4.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59DWTFp7zWag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c075f3e5-6ad4-4a69-d135-76fe759e8fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/dtypes.py:35: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
            "  from tensorflow.tsl.python.lib.core import pywrap_ml_dtypes\n",
            "ERROR:jax._src.xla_bridge:Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow_model_optimization/__init__.py:65: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.version.VERSION) <\n",
            "/usr/local/lib/python3.11/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  pkg_resources.declare_namespace(__name__)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:2563: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(parent)\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ****** Running generation 0 ****** \n",
            "\n",
            "Population's average fitness: -96.94363 stdev: 29.72825\n",
            "Best fitness: -17.74275 - size: (4, 96) - species 1 - id 2\n",
            "Average adjusted fitness: 0.291\n",
            "Mean genetic distance 1.473, standard deviation 0.286\n",
            "Population of 50 members in 1 species:\n",
            "   ID   age  size  fitness  adj fit  stag\n",
            "  ====  ===  ====  =======  =======  ====\n",
            "     1    0    50    -17.7    0.291     0\n",
            "Total extinctions: 0\n",
            "Generation time: 184.331 sec\n",
            "\n",
            " ****** Running generation 1 ****** \n",
            "\n",
            "Population's average fitness: -89.75417 stdev: 31.85856\n",
            "Best fitness: -18.10024 - size: (4, 96) - species 1 - id 2\n",
            "Average adjusted fitness: 0.400\n",
            "Mean genetic distance 1.403, standard deviation 0.315\n",
            "Population of 50 members in 1 species:\n",
            "   ID   age  size  fitness  adj fit  stag\n",
            "  ====  ===  ====  =======  =======  ====\n",
            "     1    1    50    -18.1    0.400     1\n",
            "Total extinctions: 0\n",
            "Generation time: 244.261 sec (214.296 average)\n",
            "\n",
            " ****** Running generation 2 ****** \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "import neat\n",
        "import gym\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import concurrent.futures\n",
        "import cProfile\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow_addons.layers import GroupNormalization\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Constants\n",
        "CONFIG_PATH = './neat_config.txt'\n",
        "BEST_GENOME_PATH = './best_genome.pkl'\n",
        "DEMO_FILE = './demonstrations.pkl'\n",
        "NUM_CLIENTS = 10\n",
        "MAX_GEN = 5\n",
        "NUM_ROUNDS = 5\n",
        "EPISODES_PER_EVALUATION = 25\n",
        "MAX_EPISODES = 500\n",
        "NUM_INPUTS = 24\n",
        "NUM_OUTPUTS = 4\n",
        "CHECKPOINT_DIR = './checkpoints'\n",
        "\n",
        "# Ensure checkpoint directory exists\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def precompute_neat_outputs(env, network, num_samples):\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    for _ in range(num_samples):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.clip(network.activate(state), -1, 1)\n",
        "            inputs.append(state)\n",
        "            outputs.append(action)\n",
        "            state, _, done, _ = env.step(action)\n",
        "    return np.array(inputs), np.array(outputs)\n",
        "\n",
        "# Modify the NEATMarkovLayer to accept precomputed outputs\n",
        "class NEATMarkovLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NEATMarkovLayer, self).__init__(**kwargs)\n",
        "        self.gp_model = self._create_gp_model()\n",
        "\n",
        "    def _create_gp_model(self):\n",
        "        # Define a simple sequential model as the internal model\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)  # Assuming the output is a single scalar\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.gp_model(inputs)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 1)  # Assuming the final output is a single value\n",
        "\n",
        "# Federated Learning Test\n",
        "class FederatedLearningTest:\n",
        "    def __init__(self, clients, model_fn, trainer, state, config, demonstrations):\n",
        "        self.clients = clients\n",
        "        self.model_fn = model_fn\n",
        "        self.trainer = trainer\n",
        "        self.state = state\n",
        "        self.config = config\n",
        "        self.demonstrations = demonstrations\n",
        "        self.client_learning_rates = self.adjust_learning_rates()\n",
        "\n",
        "    def run_federated_training(self, rounds=NUM_ROUNDS):\n",
        "      metrics_list = []\n",
        "      for round_num in range(rounds):\n",
        "          self.client_learning_rates = self.adjust_learning_rates()  # Adjust learning rates each round\n",
        "          with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
        "              client_data = list(executor.map(lambda client: list(collect_client_data(client[0], client[1], self.client_learning_rates[client[2]])), self.clients))\n",
        "          weights = [self.evaluate_model(client[0], client[1]) for client in self.clients]\n",
        "          total_weight = sum(weights)\n",
        "          self.normalized_weights = [weight / total_weight for weight in weights]\n",
        "          aggregated_data = self.weighted_aggregation(client_data)\n",
        "          result = self.trainer.next(self.state, aggregated_data)\n",
        "          self.state = result.state\n",
        "          metrics_list.append(result.metrics)\n",
        "          logger.info(f'Round {round_num + 1} metrics: {result.metrics}')\n",
        "          self.save_checkpoint(round_num)\n",
        "          if self.early_stopping(metrics_list):\n",
        "              logger.info(f'Early stopping at round {round_num + 1}')\n",
        "              break\n",
        "      return self.state, metrics_list\n",
        "\n",
        "    def adjust_learning_rates(self):\n",
        "        client_learning_rates = {}\n",
        "        for i, client in enumerate(self.clients):\n",
        "            performance = self.evaluate_model(client[0], client[1])\n",
        "            learning_rate = 0.1 / (1 + np.exp(-performance))  # Example adjustment based on performance\n",
        "            client_learning_rates[i] = learning_rate\n",
        "            logger.info(f'Client {i} learning rate adjusted to {learning_rate}')\n",
        "        return client_learning_rates\n",
        "\n",
        "    def early_stopping(self, metrics_list, threshold=0.01, patience=3):\n",
        "        if len(metrics_list) < patience:\n",
        "            return False\n",
        "        recent_mse = [metrics['client_work']['train']['mean_squared_error'] for metrics in metrics_list[-patience:]]\n",
        "        return np.mean(recent_mse) < threshold\n",
        "\n",
        "    def weighted_aggregation(self, client_data):\n",
        "      def aggregate_batches(batch_list, weights):\n",
        "          max_size = max(batch['x'].shape[0] for batch in batch_list)\n",
        "          agg_x = np.zeros((max_size, batch_list[0]['x'].shape[1]), dtype=np.float32)\n",
        "          agg_y = np.zeros((max_size, batch_list[0]['y'].shape[1]), dtype=np.float32)\n",
        "\n",
        "          for batch, weight in zip(batch_list, weights):\n",
        "              x_padded = np.pad(batch['x'].numpy(), ((0, max_size - batch['x'].shape[0]), (0, 0)), 'constant')\n",
        "              y_padded = np.pad(batch['y'].numpy(), ((0, max_size - batch['y'].shape[0]), (0, 0)), 'constant')\n",
        "              agg_x += x_padded * weight\n",
        "              agg_y += y_padded * weight\n",
        "\n",
        "          return {'x': agg_x, 'y': agg_y}\n",
        "\n",
        "      all_batches = list(zip(*[list(client_data[i]) for i in range(len(client_data))]))\n",
        "      aggregated_data = []\n",
        "      for batch_group in all_batches:\n",
        "          batch_dicts = [batch for batch in batch_group]\n",
        "          aggregated_batch = aggregate_batches(batch_dicts, self.normalized_weights)\n",
        "          aggregated_data.append(tf.data.Dataset.from_tensor_slices(aggregated_batch).batch(32))\n",
        "\n",
        "      return aggregated_data\n",
        "\n",
        "    def evaluate_model(self, env, network, episodes=MAX_GEN):\n",
        "        total_reward = 0\n",
        "        for _ in range(episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = np.clip(network.activate(state), -1, 1)\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "        return total_reward / episodes\n",
        "\n",
        "    def plot_metrics(self, metrics_list):\n",
        "        rounds = range(len(metrics_list))\n",
        "        mse = [metrics['client_work']['train']['mean_squared_error'] for metrics in metrics_list]\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(rounds, mse, label='Mean Squared Error')\n",
        "        plt.xlabel('Rounds')\n",
        "        plt.ylabel('Mean Squared Error')\n",
        "        plt.title('Federated Learning Training Metrics')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_rewards(self, rewards, label):\n",
        "        plt.figure(figsize=(10, 3))\n",
        "        plt.plot(range(len(rewards)), rewards, label=label)\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title(f'Model Rewards Over Episodes ({label})')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def benchmark(self, baseline_reward):\n",
        "        neat_network = neat.nn.FeedForwardNetwork.create(pickle.load(open(BEST_GENOME_PATH, 'rb')), self.config)\n",
        "        client_rewards = [self.evaluate_model(client[0], neat_network) for client in self.clients]\n",
        "        avg_reward = np.mean(client_rewards)\n",
        "\n",
        "        logger.info(f'Average reward after federated learning: {avg_reward}')\n",
        "        logger.info(f'Baseline reward: {baseline_reward}')\n",
        "\n",
        "        plt.figure(figsize=(10, 3))\n",
        "        plt.bar(['Baseline', 'Federated Learning'], [baseline_reward, avg_reward])\n",
        "        plt.ylabel('Average Reward')\n",
        "        plt.title('Benchmarking')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_client_performance(self, client_rewards):\n",
        "        plt.figure(figsize=(10, 3))\n",
        "        for i, rewards in enumerate(client_rewards):\n",
        "            plt.plot(range(len(rewards)), rewards, label=f'Client {i+1}')\n",
        "        plt.xlabel('Rounds')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('Federated Learning Client Performance')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def save_checkpoint(self, round_num):\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_round_{round_num}.pkl')\n",
        "        with open(checkpoint_path, 'wb') as f:\n",
        "            pickle.dump((self.state, self.client_learning_rates), f)\n",
        "        logger.info(f'Checkpoint saved at round {round_num} to {checkpoint_path}')\n",
        "\n",
        "    def load_checkpoint(self, round_num):\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_round_{round_num}.pkl')\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            with open(checkpoint_path, 'rb') as f:\n",
        "                self.state, self.client_learning_rates = pickle.load(f)\n",
        "            logger.info(f'Checkpoint loaded from round {round_num} from {checkpoint_path}')\n",
        "        else:\n",
        "            logger.error(f'Checkpoint file {checkpoint_path} does not exist')\n",
        "\n",
        "# Generate synthetic data with augmentation\n",
        "def generate_synthetic_data(num_samples, augment=False):\n",
        "    X = np.linspace(0, 10, num_samples).astype(np.float32).reshape(-1, 1)\n",
        "    y = (np.sin(X).ravel() + np.random.normal(0, 0.1, num_samples)).astype(np.float32).reshape(-1, 1)\n",
        "    if augment:\n",
        "        X_aug = X + np.random.normal(0, 0.1, X.shape).astype(np.float32)\n",
        "        y_aug = y + np.random.normal(0, 0.1, y.shape).astype(np.float32)\n",
        "        X = np.vstack((X, X_aug))\n",
        "        y = np.vstack((y, y_aug))\n",
        "    return X, y\n",
        "\n",
        "# Helper function to create environments and networks for each client\n",
        "def create_environment_and_network(client_id, variation, config):\n",
        "    env = gym.make('BipedalWalker-v3')\n",
        "    env._max_episode_steps = MAX_EPISODES  # Reduce max episode steps to speed up\n",
        "    env.env.gravity = variation * client_id\n",
        "    genome = load_genome(BEST_GENOME_PATH)\n",
        "    network = neat.nn.FeedForwardNetwork.create(genome, config)\n",
        "    return env, network, client_id\n",
        "\n",
        "def create_neat_config(path):\n",
        "    config_content = \"\"\"\n",
        "    [NEAT]\n",
        "    fitness_criterion = max\n",
        "    fitness_threshold = 300\n",
        "    pop_size = 50\n",
        "    reset_on_extinction = False\n",
        "    [DefaultGenome]\n",
        "    num_hidden = 0\n",
        "    num_inputs = 24\n",
        "    num_outputs = 4\n",
        "    initial_connection = full\n",
        "    activation_default = tanh\n",
        "    activation_mutate_rate = 0.1\n",
        "    activation_options = tanh relu sigmoid\n",
        "    aggregation_default = sum\n",
        "    aggregation_mutate_rate = 0.0\n",
        "    aggregation_options = sum\n",
        "    bias_init_mean = 0.0\n",
        "    bias_init_stdev = 1.0\n",
        "    bias_max_value = 30.0\n",
        "    bias_min_value = -30.0\n",
        "    bias_mutate_power = 0.5\n",
        "    bias_mutate_rate = 0.7\n",
        "    bias_replace_rate = 0.1\n",
        "    response_init_mean = 1.0\n",
        "    response_init_stdev = 0.0\n",
        "    response_max_value = 30.0\n",
        "    response_min_value = -30.0\n",
        "    response_mutate_power = 0.0\n",
        "    response_mutate_rate = 0.0\n",
        "    response_replace_rate = 0.0\n",
        "    compatibility_disjoint_coefficient = 1.0\n",
        "    compatibility_weight_coefficient = 0.5\n",
        "    conn_add_prob = 0.5\n",
        "    conn_delete_prob = 0.5\n",
        "    enabled_default = True\n",
        "    enabled_mutate_rate = 0.01\n",
        "    feed_forward = True\n",
        "    node_add_prob = 0.2\n",
        "    node_delete_prob = 0.2\n",
        "    weight_init_mean = 0.0\n",
        "    weight_init_stdev = 1.0\n",
        "    weight_max_value = 30\n",
        "    weight_min_value = -30\n",
        "    weight_mutate_power = 1.0\n",
        "    weight_mutate_rate = 0.8\n",
        "    weight_replace_rate = 0.1\n",
        "    [DefaultSpeciesSet]\n",
        "    compatibility_threshold = 3.0\n",
        "    [DefaultStagnation]\n",
        "    species_fitness_func = max\n",
        "    max_stagnation = 20\n",
        "    species_elitism = 2\n",
        "    [DefaultReproduction]\n",
        "    elitism = 2\n",
        "    survival_threshold = 0.2\n",
        "    [SteadyState]\n",
        "    replacement_rate = 0.2\n",
        "    \"\"\"\n",
        "    with open(path, 'w') as config_file:\n",
        "        config_file.write(config_content.strip())\n",
        "    logger.info(f\"Created new NEAT configuration file at {path}\")\n",
        "\n",
        "# Check and create configuration file if needed\n",
        "create_neat_config(CONFIG_PATH)\n",
        "\n",
        "try:\n",
        "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction, neat.DefaultSpeciesSet, neat.DefaultStagnation, CONFIG_PATH)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load NEAT configuration: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "def evaluate_genome(genome_id, genome, config):\n",
        "    env = gym.make('BipedalWalker-v3')\n",
        "    env._max_episode_steps = MAX_EPISODES  # Ensure max episode steps are set\n",
        "    net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
        "    fitness = 0\n",
        "    try:\n",
        "        for _ in range(EPISODES_PER_EVALUATION):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = np.clip(net.activate(state), -1, 1)\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                fitness += reward\n",
        "    except gym.error.Error as e:\n",
        "        logger.error(f\"Gym environment error during genome {genome_id} evaluation: {e}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error during genome {genome_id} evaluation: {e}\")\n",
        "    finally:\n",
        "        env.close()\n",
        "    return fitness / EPISODES_PER_EVALUATION, genome_id\n",
        "\n",
        "def evaluate_genomes(genomes, config):\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
        "        futures = {executor.submit(evaluate_genome, genome_id, genome, config): genome_id for genome_id, genome in genomes}\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            genome_id = futures[future]\n",
        "            try:\n",
        "                fitness, genome_id = future.result()\n",
        "                genome = next(genome for gid, genome in genomes if gid == genome_id)\n",
        "                genome.fitness = fitness\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error evaluating genome {genome_id}: {e}\")\n",
        "\n",
        "def model_fn():\n",
        "    neat_input = tf.keras.Input(shape=(NUM_OUTPUTS,), name='x')  # Ensure this matches the reshaped data\n",
        "    markov_output = NEATMarkovLayer()(neat_input)\n",
        "    final_output = tf.keras.layers.Dense(1, activation='sigmoid')(markov_output)\n",
        "\n",
        "    model = tf.keras.Model(inputs=neat_input, outputs=final_output)\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model=model,\n",
        "        input_spec={\n",
        "            'x': tf.TensorSpec(shape=[None, NUM_OUTPUTS], dtype=tf.float32),\n",
        "            'y': tf.TensorSpec(shape=[None, 1], dtype=tf.float32)\n",
        "        },\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
        "    )\n",
        "\n",
        "def precompute_neat_outputs(env, network, num_samples):\n",
        "    states = []\n",
        "    outputs = []\n",
        "    for _ in range(num_samples):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = network.activate(state)  # Get action from NEAT network\n",
        "            states.append(state)\n",
        "            outputs.append(action)  # Assuming action needs to be transformed for TensorFlow compatibility\n",
        "            state, _, done, _ = env.step(action)\n",
        "    return np.array(states, dtype=np.float32), np.array(outputs, dtype=np.float32)\n",
        "\n",
        "# Adjust data collection to use precomputed NEAT outputs\n",
        "def collect_client_data(environment, net, learning_rate, episodes=EPISODES_PER_EVALUATION):\n",
        "    states, actions = precompute_neat_outputs(environment, net, episodes)\n",
        "    actions = actions.reshape(-1, NUM_OUTPUTS)  # Make sure actions shape matches NUM_OUTPUTS\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({\n",
        "        'x': actions,\n",
        "        'y': actions[:, 0:1]  # Assuming your model predicts something based on actions\n",
        "    }).batch(32)\n",
        "    return dataset\n",
        "\n",
        "if not os.path.exists(DEMO_FILE):\n",
        "    env_demo = gym.make('BipedalWalker-v3')\n",
        "    demos = []\n",
        "    for _ in range(5):  # Reduce number of demonstrations to speed up\n",
        "        state = env_demo.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = env_demo.action_space.sample()  # Random actions as placeholders\n",
        "            next_state, _, done, _ = env_demo.step(action)\n",
        "            demos.append((state, [action]))  # Ensure action is wrapped in a list\n",
        "            state = next_state\n",
        "    with open(DEMO_FILE, 'wb') as f:\n",
        "        pickle.dump(demos, f)\n",
        "demonstrations = pickle.load(open(DEMO_FILE, 'rb'))\n",
        "\n",
        "def load_genome(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"Error: File '{filepath}' not found. Exiting.\")\n",
        "        raise SystemExit\n",
        "\n",
        "def evaluate_with_demos(genomes, config, env, demonstrations):\n",
        "    for genome_id, genome in genomes:\n",
        "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
        "        fitness = 0\n",
        "        for _ in range(EPISODES_PER_EVALUATION):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = np.clip(net.activate(state), -1, 1)\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                fitness += reward\n",
        "        genome.fitness = fitness / EPISODES_PER_EVALUATION\n",
        "\n",
        "def federated_train(num_clients=NUM_CLIENTS, num_rounds=NUM_ROUNDS):\n",
        "    clients = [create_environment_and_network(i, 1.0 + 0.1 * i, config) for i in range(num_clients)]\n",
        "    train_data = [collect_client_data(client[0], client[1], 0.1) for client in clients if collect_client_data(client[0], client[1], 0.1) is not None]\n",
        "    model = model_fn()\n",
        "    trainer = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn=model_fn,\n",
        "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "    )\n",
        "    state = trainer.initialize()\n",
        "    metrics_list = []\n",
        "    for round_num in range(num_rounds):\n",
        "        result = trainer.next(state, train_data)\n",
        "        state = result.state\n",
        "        metrics = result.metrics\n",
        "        metrics_list.append(metrics)\n",
        "        logger.info(f'Round {round_num + 1}: {metrics[\"client_work\"][\"train\"][\"mean_squared_error\"]}')\n",
        "    return state, metrics_list\n",
        "\n",
        "def train_neat_non_federated(config, generations=MAX_GEN):\n",
        "    env = gym.make('BipedalWalker-v3')\n",
        "    env._max_episode_steps = MAX_EPISODES\n",
        "    population = neat.Population(config)\n",
        "    population.add_reporter(neat.StdOutReporter(True))\n",
        "    stats = neat.StatisticsReporter()\n",
        "    population.add_reporter(stats)\n",
        "\n",
        "    for generation in range(generations):\n",
        "        # Evaluate the population\n",
        "        genomes = list(population.population.items())\n",
        "        logger.info(f'Starting generation {generation + 1}/{generations}')\n",
        "        population.run(lambda genomes, config: evaluate_genomes(genomes, config), 1)\n",
        "        best_genome = population.best_genome\n",
        "        if best_genome.fitness >= config.fitness_threshold:\n",
        "            logger.info(f'Early stopping at generation {generation + 1}')\n",
        "            break\n",
        "\n",
        "    return population.best_genome, stats\n",
        "\n",
        "def plot_comparison(federated_rewards, non_federated_rewards):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    episodes = range(len(federated_rewards))\n",
        "    plt.plot(episodes, federated_rewards, label='Federated NEAT')\n",
        "    plt.plot(episodes, non_federated_rewards, label='Non-Federated NEAT')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title('Comparison of Federated and Non-Federated NEAT')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Profile the script to find bottlenecks\n",
        "    profiler = cProfile.Profile()\n",
        "    profiler.enable()\n",
        "\n",
        "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction, neat.DefaultSpeciesSet, neat.DefaultStagnation, CONFIG_PATH)\n",
        "\n",
        "    # Create a population\n",
        "    population = neat.Population(config)\n",
        "    population.add_reporter(neat.StdOutReporter(True))\n",
        "    stats = neat.StatisticsReporter()\n",
        "    population.add_reporter(stats)\n",
        "\n",
        "    # Evaluate the population\n",
        "    genomes = list(population.population.items())\n",
        "\n",
        "    if not os.path.exists(BEST_GENOME_PATH):\n",
        "        winner = population.run(lambda genomes, config: evaluate_genomes(genomes, config), MAX_GEN)\n",
        "        with open(BEST_GENOME_PATH, 'wb') as f:\n",
        "            pickle.dump(winner, f)\n",
        "\n",
        "    winner = pickle.load(open(BEST_GENOME_PATH, 'rb'))\n",
        "    winner, stats = train_neat_non_federated(config, MAX_GEN)\n",
        "    with open(BEST_GENOME_PATH, 'wb') as f:\n",
        "        pickle.dump(winner, f)\n",
        "\n",
        "    clients = [create_environment_and_network(i, 1.0 + 0.1 * i, config) for i in range(NUM_CLIENTS)]\n",
        "    trainer = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn=model_fn,\n",
        "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "    )\n",
        "    state = trainer.initialize()\n",
        "\n",
        "    test = FederatedLearningTest(clients, model_fn, trainer, state, config, demonstrations)\n",
        "    state, federated_metrics = test.run_federated_training(NUM_ROUNDS)\n",
        "\n",
        "    neat_network = neat.nn.FeedForwardNetwork.create(pickle.load(open(BEST_GENOME_PATH, 'rb')), config)\n",
        "    federated_rewards = [test.evaluate_model(client[0], neat_network) for client in clients]\n",
        "    non_federated_rewards = [test.evaluate_model(client[0], neat_network, MAX_GEN) for client in clients]\n",
        "\n",
        "    plot_comparison(federated_rewards, non_federated_rewards)\n",
        "\n",
        "    client_rewards = []\n",
        "    for client in clients:\n",
        "        client_rewards.append([test.evaluate_model(client[0], neat_network) for _ in range(NUM_ROUNDS)])\n",
        "    test.plot_client_performance(client_rewards)\n",
        "\n",
        "    test.plot_metrics(federated_metrics)\n",
        "\n",
        "    # Disable profiler and print profiling results\n",
        "    profiler.disable()\n",
        "    profiler.print_stats(sort='cumtime')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMSMJWK9cCL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}